{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computación Avanzada y sus Aplicaciones a Ingeniería\n",
    "\n",
    "### Máster Universitario en Ingeniería Informática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1 - Parte I - Introducción a la programación en Apache Spark\n",
    "\n",
    "En esta práctica introducimos las operaciones básicas para trabajar con los RDDs de Spark. Este primer notebook no es más que una guía de todas las operaciones que puedes realizar en Spark. Sigue detenidamente todos los bloques y prueba a cambiar los valores establecidos para comprobar su funcionamiento.\n",
    "\n",
    "Ten en cuenta que una vez tengas en marcha Spark, podrás visualizar la evolución de cada trabajo de Spark en  <http://localhost:4040>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Uso básico de los notebooks y su integración con Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilización de un Notebook\n",
    "\n",
    "Un notebook está compuesto por una serie de celdas. Estas celdas pueden contener texto explicativo o código, pero nunca se mezclan ambas en la misma celda. Cuando ejecutamos una celda de texto, lo que hemos escrito con el lenguaje de markdown se renderiza como texto, imágenes y links (como si fuera HTML). El texto que estás leyendo ahora mismo es parte de una celda de este tipo. Las celdas con código Python te permiten ejecutar comandos de Python como si estuvieras en la consola de Python. Coloca el cursos dentro de la celda de más abajo y presiona \"Shift + Enter\" para ejecutar el código y avanzar a la siguiente celda. También puedes utilizar \"Ctrl + Enter\" para ejecutar el código y mantenerte en la misma celda. Estos comandos funcionan tanto en celdas de código como en celdas de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La suma de 1 y 1 es 2\n"
     ]
    }
   ],
   "source": [
    "# Esto es una celda ed Python. Puedes ejecutar código Python en estas celdas\n",
    "print('La suma de 1 y 1 es {0}'.format(1+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x es mayor que 18\n"
     ]
    }
   ],
   "source": [
    "# Esta es otra celda Python, utiliza una variable x y un if\n",
    "x = 28\n",
    "if x > 18:\n",
    "    print('x es mayor que 18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estado de un Notebook\n",
    "\n",
    "Cuando trabajas con un notebook es importante ejecutar todas las celdas con código. El notebook tiene estado, lo que quiere decir que las variables y sus valores se mantienen hasta el que el kernel del notebook se reinicia. Si no ejecutas todas las celdas de código a lo largo del notebook, las variables pueden no estar correctamente inicializadas y pueden fallar celdas de código posteriores. También necesitarás reejecutar cualquier celda que hayas modificado para que los cambios estén disponibles en otras celdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "# Esta celda utiliza la variable x que hemos definido en una celda anterior\n",
    "# Si no ejecutamos la celda anaterior este código fallará\n",
    "print(x * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de estar utilizando pySpark, **NO** es necesario inicializar el `SparkContext`, es decir, **no** ejecutar la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/20 10:45:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/20 10:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de un RDD\n",
    "Podemos crear un RDD de dos formas\n",
    "\n",
    "1. Cargando un conjunto de datos almacenado en un medio externo: `sc.textFile(fichero)`\n",
    "2. Distribuyendo una colección de objetos existente: `sc.parallelize(colección_python)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un RDD desde un fichero\n",
    "quijoteRDD = sc.textFile(\"./datos/pg2000.txt\")\n",
    "\n",
    "# Creación de un RDD desde una colección\n",
    "datos = [1, 2, 3, 4, 5]\n",
    "datoRDD = sc.parallelize(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos realizar dos tipos de operaciones sobre los RDDs\n",
    "1. Transformaciones: Crean un nuevo RDD a partir de otro - **EVALUACIÓN VAGA (LAZY)** - hasta que no se ejecuta una acción no se realiza la transformación\n",
    "2. Acciones: Utilizan el RDD para lograr un resultado que es recibido por el driver (o escriben el RDD a disco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones sobre RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones básicas\n",
    "\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*map(func)* | Devuelve un nuevo RDD formado aplicando a cada elemento del RDD original la función func\n",
    "*filter(func)* | Devuelve un nuevo RDD formado por los elementos para los cuales el aplicarles la función func devuelve true\n",
    "*distinct()* | Devuelve un nuevo RDD que contiene los elementos distintos dentro del RDD original\n",
    "*flatMap(func)* | Similar al map, pero cada elemento de entrada puede ser mapeado a 0 o más elementos de salida (por tanto, func devuelve una secuencia en vez de un único elemento)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `map(func)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "rdd.map(lambda x: x * 2).collect() # Utilizamos collect() para ver el resultado en el driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `filter(func)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(lambda x: x != 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `distinct()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4, 1, 2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 4, 2, 2, 3])\n",
    "rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `map(func)` vs `flatMap(func)`\n",
    "Es importante diferenciar la función `map`de `flatMap`. `map` devuelve tantos elementos como tiene el RDD original, mientras que en `flatMap` la función debe devolver una lista de elementos (que puede ser vacía o tener más de un elemento) y concatena todas las listas en un único RDD de elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3])\n",
    "print(\"map: \" + str(rdd.map(lambda x: [x, x+5]).collect()))\n",
    "print(\"flatMap: \" + str(rdd.flatMap(lambda x: [x, x+5]).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize([\"hello world\", \"hi\", \"dime tu nombre\", \"hasta luego\"])\n",
    "\n",
    "wordsMap = lines.map(lambda line: line.split(\" \"))\n",
    "wordsFlatMap = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "print(\"map: \" + str(wordsMap.collect()))\n",
    "print(\"flatMap: \" + str(wordsFlatMap.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones con pseudo-conjuntos\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*distinct()* | Devuelve el RDD sin elementos repetidos – ¡Cuidado! Requiere shuffle (enviar datos por red)\n",
    "*union(rdd)* | Devuelve la unión de los elementos en los dos RDDs  (se mantienen los duplicados)\n",
    "*intersection(rdd)* | Devuelve la instersección de los elementos en los dos RDDs (elimina los duplicados) – ¡Cuidado! Requiere shuffle (datos por red)\n",
    "*subtract(rdd)* | Devuelve los elementos presentes en el primer RDD y no en el segundo – ¡Cuidado! También requiere de shuffle\n",
    "*cartesian(rdd)* | Devuelve un RDD con todos los posibles pares entre elementos de ambos RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([\"agua\", \"vino\", \"cerveza\", \"agua\", \"agua\", \"vino\"])\n",
    "rdd2 = sc.parallelize([\"cerveza\", \"cerveza\", \"agua\", \"agua\", \"vino\", \"coca-cola\", \"naranjada\"])\n",
    "\n",
    "print(\"distinct: \" + str(rdd1.distinct().collect()))\n",
    "print(\"union: \" + str(rdd1.union(rdd2).collect()))\n",
    "print(\"intersection: \" + str(rdd1.intersection(rdd2).collect()))\n",
    "print(\"substract: \" + str(rdd1.subtract(rdd2).collect()))\n",
    "print(\"cartesian: \" + str(rdd1.cartesian(rdd2).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones sobre RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones básicas\n",
    "\n",
    "\n",
    "Acción | Descripción\n",
    "------------- | -------------\n",
    "reduce(func) | Agrega los elementos del RDD usando la función func. func toma dos argumentos y devuelve uno, y es conmutativa y asociativa de tal forma que puede calcularse correctamente en paralelo\n",
    "*take(n)* | Devuelve una lista con los n primeros elementos del RDD\n",
    "*collect()* | Devuelve todos los elementos del RDD como una lista. **CUIDADO: Hay que asegurarse de que vayan a caber en el driver**\n",
    "*takeOrdered(n, key=func)* | Devuelve n elementos ordenados de manera ascendente o en el orden especificado por la función de orden opcional func\n",
    "*foreach(func)* | Aplica la función func a cada elemento del RDD. No devuelve nada, puede valer para realizar inserciones a BBDD por ejemplo\n",
    "*count()* | Cuenta el número de elementos en el RDD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "prod = rdd.reduce(lambda a, b: a * b)\n",
    "print(\"Resultado del producto: \" + str(prod))  # función conmutativa y asociativa!!!\n",
    "print(\"Dos primeros valores con take(2): \" + str( rdd.take(2) ))\n",
    "print(\"Todo el RDD con collect(): \" + str( rdd.collect() ))\n",
    "\n",
    "rdd = sc.parallelize([5, 3, 1, 2]) \n",
    "print(\"Los tres elementos más grandes con takeOrdered(3, func): \" + str( rdd.takeOrdered(3, lambda s: -1 * s) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones clave-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones básicas\n",
    "\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "reduceByKey(func)  | Devuelve un nuevo RDD de tuplas (K, V) donde los valores para cada clave K son agregados usando una función de reducción func, cuyo tipo debe ser (V, V) à V\n",
    "*sortByKey()* | Devuelve un nuevo RDD de tuplas (K, V) ordenadas por clave en orden ascendente\n",
    "*groupByKey() * | Devuelve un nuevo RDD de tuplas (K, iterable(V)) **¡Cuidado! Puede ser muy costoso – datos por red**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)]) \n",
    "print(\"reduceByKey: \" + str( rdd.reduceByKey(lambda a, b: a + b).collect() ))\n",
    "\n",
    "rdd2 = sc.parallelize([(1,'a'), (2,'c'), (1,'b')])\n",
    "print(\"reduceByKey: \" + str( rdd2.sortByKey().collect() ))\n",
    "    \n",
    "rdd2 = sc.parallelize([(1,'a'), (2,'c'), (1,'b')]) \n",
    "print(\"reduceByKey: \" + str( rdd2.groupByKey().collect() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones Join tipo SQL\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "join(rdd)  | Inner join entre dos RDDs\n",
    "*leftOuterJoin(rdd)* | Realiza un join entre los dos RDDs donde la clave debe estar presente en el segundo de ellos\n",
    "*rightOuterJoin(rdd)* | Realiza un join entre los dos RDDs donde la clave debe estar presente en el primero de ellos\n",
    "*fullOuterJoin(rdd)* | Realiza un join entre los dos RDDs donde la clave debe estar presente alguno de ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `join`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `leftOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "sorted(x.leftOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rightOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "sorted(x.rightOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fullOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
    "sorted(x.fullOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones clave-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones básicas\n",
    "\n",
    "\n",
    "Acción | Descripción\n",
    "------------- | -------------\n",
    "countByKey() | Cuenta el número de elementos para cada clave\n",
    "*collectAsMap()* | Recolecta el RDD como un Map para facilitar las búsquedas\n",
    "*lookup(key)* | Devuelve el valor asociado con la clave dada\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "\n",
    "print(\"countByKey: \" + str( rdd.countByKey() ))\n",
    "print(\"collectAsMap: \" + str( rdd.collectAsMap() ))\n",
    "print(\"lookup(3): \" + str( rdd.lookup(3) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caché de RDDs\n",
    "Si se va a reusar un RDD es conveniente cachearlo para que no se recalcule cada vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quijoteRDD = sc.textFile(\"./datos/pg2000.txt\")\n",
    "palabrasQuijoteRDD = quijoteRDD.flatMap(lambda line: line.split(' ')).cache()\n",
    "print(\"Cabeza aparece \" + str( quijoteRDD.filter(lambda line: \"cabeza\" in line).count() ) + \" veces\")\n",
    "print(\"Lanza aparece \" + str( quijoteRDD.filter(lambda line: \"Lanza\" in line).count() ) + \" veces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspectos avanzados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables broadcast\n",
    "Permiten enviar eficientemente valores de gran tamaño a los workers (solo lectura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablaLookUp = {1: \"a\", 2: \"b\", 3: \"c\", 4: \"d\"} # suponer que es muy grande\n",
    "\n",
    "tablaLookUp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablaLookUp = {1: \"a\", 2: \"b\", 3: \"c\", 4: \"d\"} # suponer que es muy grande\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "\n",
    "sinBroadcast = rdd.map(lambda v: tablaLookUp[v]).collect()\n",
    "\n",
    "tablaLookUpBroadcast = sc.broadcast(tablaLookUp) # creamos la variable tipo broadcast que se distribuye a los workers\n",
    "conBroadcast = rdd.map(lambda v: tablaLookUpBroadcast.value[v]).collect() # con .value accedemos al valor de la variable broadcast\n",
    "\n",
    "print(\"Sin broadcast: \" + str(sinBroadcast))\n",
    "print(\"Con broadcast: \" + str(conBroadcast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acumuladores\n",
    "Agregan valores del os executors en el driver. Solo el driver puede leer las variables, para los workers son solo de escritura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "\n",
    "def f(x):\n",
    "    global accum \n",
    "    accum += x\n",
    "    \n",
    "rdd.foreach(f)\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quijoteRDD = sc.textFile(\"./datos/pg2000.txt\")\n",
    "#Creamos un Accumulator[Int] inicializado a 0\n",
    "blankLines = sc.accumulator(0)\n",
    "\n",
    "def extraePalabrasBlankLines(line):\n",
    "    global blankLines # Hacemos la variable global accesible\n",
    "    if (line == \"\"):\n",
    "        blankLines += 1\n",
    "    return line.split(\" \")\n",
    "    \n",
    "palabrasQuijoteRDD = quijoteRDD.flatMap(extraePalabrasBlankLines)\n",
    "# Provocamos que se ejecute la transformación\n",
    "palabrasQuijoteRDD.count()\n",
    "print(\"Líneas en blancos: %d\" % blankLines.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajar con datos por particiones\n",
    "Trabajamos con todos los datos de una partición a la vez.\n",
    "En vez de aplicar una función por elemento se aplica una función para el iterador de elementos de la partición.\n",
    "Permite evitar rehacer trabajos de configuración con cada elemento o trabajar con todos los elementos a la vez.\n",
    "\n",
    "Transformación / Acción | Descripción\n",
    "------------- | -------------\n",
    "*mapPartitions(func)* | Aplica la función func a cada partición del RDD. La función func recibe un iterador de elementos y devuelve otro iterador con elementos que pueden ser de diferente tipo\n",
    "*mapPartitionsWithIndex(func)* | Aplica la función func a cada partición del RDD. La función func recibe una tupla (entero, iterador) donde el entero representa el índice de la partición y el iterador contiene todos los elementos de la partición.\n",
    "*foreachPartition(func)* | Aplica la función func a cada partición del RDD. No devuelve nada. Puede usarse para realizar inserciones en una BBDD por ejemplo. func recibe un iterador de elementos y no devuelve nada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media sin `mapPartitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCtrs(c1, c2):\n",
    "    return (c1[0] + c2[0], c1[1] + c2[1])\n",
    "\n",
    "def basicAvg(nums):\n",
    "    \"\"\"Compute the average\"\"\"\n",
    "    sumCount = nums.map(lambda num: (num, 1)).reduce(combineCtrs)\n",
    "    return sumCount[0] / float(sumCount[1])\n",
    "\n",
    "a = sc.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "basicAvg(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media con `mapPartitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitionCtr(nums):\n",
    "    \"\"\"Compute sumCounter for partition\"\"\"\n",
    "    sumCount = [0, 0]\n",
    "    for num in nums:\n",
    "        sumCount[0] += num\n",
    "        sumCount[1] += 1\n",
    "    return [sumCount]\n",
    "\n",
    "def fastAvg(nums):\n",
    "    \"\"\"Compute the avg\"\"\"\n",
    "    sumCount = nums.mapPartitions(partitionCtr).reduce(combineCtrs)\n",
    "    return sumCount[0] / float(sumCount[1])\n",
    "\n",
    "a = sc.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "fastAvg(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mapPartitionsWithIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel = sc.parallelize(range(1, 10), 3)\n",
    "def show(index, iterator): \n",
    "    return ['index: ' + str(index) + \" values: \" + str(list(iterator))] # debemos devolver una lista ya que requiere un iterador\n",
    "\n",
    "parallel.mapPartitionsWithIndex(show).collect()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones con RDD numéricos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método `stats()`\n",
    "Método `stats()` devuelve un `StatsCounter` con todas las estadísticas calculadas mediante una única pasada por todo el RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "rdd.stats() # devuelve un StatsCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos propios sobre el RDD\n",
    "\n",
    "Método (acción) | Descripción\n",
    "------------- | -------------\n",
    "*Método* | Descripción\n",
    "*count()* | Número de elementos en el RDD\n",
    "*mean()* | Media de los elementos en el RDD\n",
    "*sum()* | Suma total de los elementos en el RDD\n",
    "*max()* | Máximo valor\n",
    "*min()* | Mínimo valor\n",
    "*variance()* | Varianza de los elementos\n",
    "*sampleVariance()* | Variance de los elementos calculada para una muestra\n",
    "*stdev()* | Desviación estándar de los elementos\n",
    "*sampleStdev()* | Desviación estándar para una muestra\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "# ESTA FORMA ES MUCHO MENOS ÓPTIMA QUE STATS() DEBIDO A QUE SE REALIZA UNA PASADA POR EL DATASET PARA CADA ESTADÍSTICA\n",
    "print(\"Count: \" + str(rdd.count()))\n",
    "print(\"Mean: \" + str(rdd.mean()))\n",
    "print(\"Sum: \" + str(rdd.sum()))\n",
    "print(\"Max: \" + str(rdd.max()))\n",
    "print(\"Min: \" + str(rdd.min()))\n",
    "print(\"Variance: \" + str(rdd.variance()))\n",
    "print(\"Smaple variance: \" + str(rdd.sampleVariance()))\n",
    "print(\"Standard deviation: \" + str(rdd.stdev()))\n",
    "print(\"Sample standard deviation: \" + str(rdd.sampleStdev()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y escritura de ficheros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ficheros de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = sc.textFile(\"datos/pg2000.txt\")\n",
    "input2 = sc.textFile(\"datos/\")\n",
    "input3 = sc.textFile(\"datos/*.txt\")\n",
    "input4 = sc.wholeTextFiles(\"datos/*.txt\")\n",
    "\n",
    "print(\"Elementos en RDD a partir de pg2000.txt: \" + str(input1.count()))\n",
    "# print \"Elementos en RDD a partir de datos/: \" + str(input2.count())    #  datasets muy grandes - mejor no esperar\n",
    "print(\"Elementos en RDD a partir de datos/*.txt: \" + str(input3.count()))\n",
    "print(\"Elementos en RDD a partir de datos/.*txt con wholeTextFiles: \" + str(input4.count()))\n",
    "\n",
    "# CUIDADO, Falla si la carpeta ya existe\n",
    "input1.saveAsTextFile(\"datos/salida\")\n",
    "print(\"Ver datos escritos en datos/salida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ficheros JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = sc.textFile(\"datos/json.json\").map(lambda x: json.loads(x))\n",
    "print(\"Elementos en RDD a partir de datos/json.json: \" + str(data.count()))\n",
    "\n",
    "\n",
    "data.map(lambda x: json.dumps(x)).saveAsTextFile(\"datos/salida.json\")\n",
    "print(\"Ver datos escritos en datos/salida.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ficheros CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import StringIO\n",
    "def loadRecord(line):\n",
    "    \"\"\"Parse a CSV line\"\"\"\n",
    "    input = StringIO.StringIO(line)\n",
    "    reader = csv.DictReader(input, fieldnames=[\"nombre\", \"tel\", \"email\"])\n",
    "    return reader.next()\n",
    "\n",
    "inputCSV = sc.textFile(\"datos/personas.csv\").map(loadRecord)\n",
    "print(\"Primeras 10 personas: \")\n",
    "print(str(inputCSV.take(10)))\n",
    "\n",
    "\n",
    "def writeRecords(records):\n",
    "    \"\"\"Write out CSV lines\"\"\"\n",
    "    output = StringIO.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=[\"nombre\", \"tel\", \"email\"])\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]\n",
    "    \n",
    "inputCSV.mapPartitions(writeRecords).saveAsTextFile(\"datos/salida.csv\")\n",
    "print(\"Ver datos escritos en datos/salida.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
